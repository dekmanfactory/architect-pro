1
00:00:01,000 --> 00:00:05,500
[1차수: 신경망 깊게 파고들기] 심화 과정 첫 시간입니다. 신경망의 학습 원리인 역전파를 알아봅니다.

2
00:00:06,000 --> 00:00:10,500
출력값의 오차를 뒤로 전달하며 가중치를 업데이트하는 과정, 이것이 딥러닝의 핵심 메커니즘이죠.

3
00:03:01,000 --> 00:03:05,500
[2차수: CNN의 원리] 2차수에서는 이미지 처리에 특화된 합성곱 신경망, CNN을 다룹니다.

4
00:03:06,000 --> 00:03:11,000
필터를 통해 이미지의 특징을 추출하고 공간적 구조를 유지하는 방식에 대해 학습하겠습니다.

5
00:06:01,000 --> 00:06:05,500
[3차수: RNN과 시계열 데이터] 순차적 데이터를 다루는 RNN은 이전의 정보를 기억하는 능력이 있습니다.

6
00:06:06,000 --> 00:06:11,000
기울기 소실 문제를 해결한 LSTM과 GRU의 구조적 차이점까지 심도 있게 분석해 봅시다.

7
00:09:01,000 --> 00:09:05,500
[4차수: 자연어 처리(NLP) 기초] 컴퓨터가 인간의 언어를 이해하도록 만드는 NLP의 기본을 배웁니다.

8
00:09:06,000 --> 00:09:11,000
단어를 숫자로 바꾸는 워드 임베딩 기술이 어떻게 의미적 유사도를 계산하는지 확인해 보세요.

9
00:12:01,000 --> 00:12:05,500
[5차수: 어텐션 메커니즘] 모델이 입력 데이터 중 어디에 '집중'할지를 결정하는 어텐션 기술입니다.

10
00:12:06,000 --> 00:12:11,000
이 기술의 등장이 번역 및 문장 생성의 품질을 어떻게 비약적으로 높였는지 살펴봅니다.

11
00:15:01,000 --> 00:15:05,500
[6차수: 트랜스포머 아키텍처] 현재 모든 거대 언어 모델의 근간이 된 트랜스포머 구조를 분석합니다.

12
00:15:06,000 --> 00:15:11,000
Self-Attention을 통해 병렬 처리를 가능케 한 혁신적인 설계 방식을 심층 탐구합니다.

13
00:18:01,000 --> 00:18:05,500
[7차수: GAN - 생성적 적대 신경망] 이미지를 창조하는 AI, GAN의 원리를 알아봅니다.

14
00:18:06,000 --> 00:18:11,000
생성자와 판별자가 서로 경쟁하며 가짜를 진짜처럼 만들어가는 흥미로운 과정을 다룹니다.

15
00:21:01,000 --> 00:21:05,500
[8차수: 전이 학습과 미세 조정] 바닥부터 학습하는 대신, 잘 만들어진 모델을 빌려 쓰는 전이 학습입니다.

16
00:21:06,000 --> 00:21:11,000
적은 데이터로도 고성능 모델을 만들 수 있는 Fine-tuning의 실전 기법을 배웁니다.

17
00:24:01,000 --> 00:24:05,500
[9차수: 모델 최적화와 경량화] 복잡한 모델을 스마트폰에서도 돌릴 수 있을까요? 경량화가 답입니다.

18
00:24:06,000 --> 00:24:11,000
가지치기와 양자화를 통해 성능은 유지하면서 크기를 줄이는 최신 기법을 소개합니다.

19
00:27:01,000 --> 00:27:05,500
[10차수: 심화 과정 마무리] 수고하셨습니다. 이제 여러분은 현대 AI의 주요 구조를 이해하게 되었습니다.

20
00:27:06,000 --> 00:27:11,000
기술적 이해를 바탕으로 실무에서 AI를 어떻게 아키텍처링할지 고민하는 시간을 가져보세요.