1
00:00:01,000 --> 00:00:05,500
[1차수: 딥러닝의 심장, 퍼셉트론] 안녕하세요! AI 심화 과정의 첫 시작, 딥러닝의 기본 단위인 퍼셉트론을 분석합니다.

2
00:00:06,000 --> 00:00:11,000
입력값과 가중치의 곱, 그리고 편향이 더해져 하나의 신호를 만드는 수학적 구조를 완벽히 이해해 봅시다.

3
00:03:01,000 --> 00:03:05,500
[2차수: 다층 퍼셉트론(MLP)의 등장] 단층의 한계를 넘어, 여러 개의 은닉층을 쌓아 복잡한 문제를 해결하는 과정을 배웁니다.

4
00:03:06,000 --> 00:03:11,000
비선형 문제를 해결하기 위해 층을 쌓는 것이 왜 중요한지 XOR 문제를 통해 확인해 보겠습니다.

5
00:06:01,000 --> 00:06:05,500
[3차수: 활성화 함수의 역할] 시그모이드부터 ReLU까지, 신경망에 생명력을 불어넣는 활성화 함수를 비교합니다.

6
00:06:06,000 --> 00:06:11,000
기울기 소실 문제를 해결하기 위해 현대 딥러닝에서 ReLU가 표준이 된 이유를 심도 있게 다룹니다.

7
00:09:01,000 --> 00:09:05,500
[4차수: 손실 함수와 경사하강법] 모델이 얼마나 틀렸는지 측정하는 손실 함수와 최적의 답을 찾아가는 경사하강법입니다.

8
00:09:06,000 --> 00:09:11,000
평균제곱오차(MSE)와 크로스 엔트로피의 차이를 이해하고 오차를 최소화하는 수학적 최적화를 실습합니다.

9
00:12:01,000 --> 00:12:05,500
[5차수: 역전파(Backpropagation) 마스터] 딥러닝 학습의 핵심인 역전파 알고리즘의 미분 연산 과정을 파헤칩니다.

10
00:12:06,000 --> 00:12:11,000
연쇄 법칙(Chain Rule)을 통해 출력층의 오차가 어떻게 입력층까지 전달되는지 시각적으로 풀어드립니다.

11
00:15:01,000 --> 00:15:05,500
[6차수: 옵티마이저(Optimizer) 비교] Adam, RMSprop 등 경사하강법을 더 빠르고 똑똑하게 만드는 최적화 도구들입니다.

12
00:15:06,000 --> 00:15:11,000
각 알고리즘의 수식을 비교하며 데이터의 특성에 맞는 옵티마이저를 선택하는 기준을 세워봅니다.

13
00:18:01,000 --> 00:18:05,500
[7차수: 과적합 방지와 규제화] 모델이 학습 데이터에만 너무 집착하는 '과적합' 현상을 막는 기술입니다.

14
00:18:06,000 --> 00:18:11,000
드롭아웃(Dropout)과 L1/L2 규제가 모델의 일반화 성능을 어떻게 높여주는지 확인해 봅니다.

15
00:21:01,000 --> 00:21:05,500
[8차수: 배치 정규화(Batch Normalization)] 학습 속도를 높이고 초기화의 영향을 줄여주는 배치 정규화의 원리입니다.

16
00:21:06,000 --> 00:21:11,000
각 층의 입력을 평균 0, 분산 1로 조절하여 딥러닝 모델을 더 안정적으로 학습시키는 법을 배웁니다.

17
00:24:01,000 --> 00:24:05,500
[9차수: 하이퍼파라미터 튜닝] 학습률, 배치 크기, 은닉층 수 등 정답이 없는 변수들을 찾는 전략입니다.

18
00:24:06,000 --> 00:24:11,000
그리드 서치와 베이지안 최적화 기법을 통해 효율적으로 최고의 성능을 이끌어내는 노하우를 공유합니다.

19
00:27:01,000 --> 00:27:05,500
[10차수: 심화 이론 요약 및 다음 단계] 딥러닝의 거대한 수학적 지도를 함께 그려보았습니다.

20
00:27:06,000 --> 00:27:11,000
이 탄탄한 기초 위에서 다음 강의인 CNN과 RNN 같은 특수 목적형 아키텍처로 넘어가겠습니다. 수고하셨습니다.